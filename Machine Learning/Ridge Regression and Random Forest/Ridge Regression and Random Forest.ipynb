{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518808bb",
   "metadata": {},
   "source": [
    "### 1. Implemented a regularised ridge regression model trained with SGD to create a linear predictive model\n",
    "\n",
    "\n",
    "Our predictive model is going to be a linear model\n",
    "\n",
    "$$ f(\\mathbf{x}_i) = \\mathbf{w}^{\\top}\\mathbf{x}_i,$$\n",
    "\n",
    "where $\\mathbf{w} = [w_0\\; w_1\\; \\cdots \\; w_D]^{\\top}$.\n",
    "\n",
    "The **objective function** we are going to use has the following form\n",
    "\n",
    "$$ E(\\mathbf{w}, \\lambda) = \\frac{1}{N}\\sum_{n=1}^N (y_n - f(\\mathbf{x}_n))^2 + \\frac{\\lambda}{2}\\sum_{j=0}^D w_j^2,$$\n",
    "\n",
    "where $\\lambda>0$ is known as the *regularisation* parameter.\n",
    "\n",
    "and the update equation for $\\mathbf{w}_{\\text{new}}$ using gradient descent:\n",
    "\n",
    "\\begin{align*}\n",
    "   \\mathbf{w}_{\\text{new}} & = (1 - \\eta\\lambda)\\mathbf{w}_{\\text{old}} + \\frac{2\\eta}\n",
    "                               {N}\\sum_{n=1}^N   \n",
    "                               \\left(y_n - \\mathbf{x}_n^{\\top}\\mathbf{w}_{\\text{old}}\\right)\\mathbf{x}_n\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9ab22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3b494",
   "metadata": {},
   "source": [
    "#### Acquiring Air Quality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4169de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Airquality data\n",
    "doc = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00360/AirQualityUCI.zip\"\n",
    "pat_sav = \"./AirQualityUCI.zip\"\n",
    "urllib.request.urlretrieve(doc, pat_sav)\n",
    "\n",
    "#Extracting data\n",
    "zip = zipfile.ZipFile('./AirQualityUCI.zip', 'r')\n",
    "for name in zip.namelist():\n",
    "    zip.extract(name, '.')\n",
    "    \n",
    "#importing data\n",
    "air_quality_full = pd.read_excel('./AirQualityUCI.xlsx', usecols=range(2,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf681b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO(GT)</th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>NMHC(GT)</th>\n",
       "      <th>C6H6(GT)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>NOx(GT)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>NO2(GT)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>T</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.6</td>\n",
       "      <td>1360.00</td>\n",
       "      <td>150</td>\n",
       "      <td>11.881723</td>\n",
       "      <td>1045.50</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1056.25</td>\n",
       "      <td>113.0</td>\n",
       "      <td>1692.00</td>\n",
       "      <td>1267.50</td>\n",
       "      <td>13.60</td>\n",
       "      <td>48.875001</td>\n",
       "      <td>0.757754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1292.25</td>\n",
       "      <td>112</td>\n",
       "      <td>9.397165</td>\n",
       "      <td>954.75</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1173.75</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1558.75</td>\n",
       "      <td>972.25</td>\n",
       "      <td>13.30</td>\n",
       "      <td>47.700000</td>\n",
       "      <td>0.725487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.2</td>\n",
       "      <td>1402.00</td>\n",
       "      <td>88</td>\n",
       "      <td>8.997817</td>\n",
       "      <td>939.25</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1140.00</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1554.50</td>\n",
       "      <td>1074.00</td>\n",
       "      <td>11.90</td>\n",
       "      <td>53.975000</td>\n",
       "      <td>0.750239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.2</td>\n",
       "      <td>1375.50</td>\n",
       "      <td>80</td>\n",
       "      <td>9.228796</td>\n",
       "      <td>948.25</td>\n",
       "      <td>172.0</td>\n",
       "      <td>1092.00</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1583.75</td>\n",
       "      <td>1203.25</td>\n",
       "      <td>11.00</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.786713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.6</td>\n",
       "      <td>1272.25</td>\n",
       "      <td>51</td>\n",
       "      <td>6.518224</td>\n",
       "      <td>835.50</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1205.00</td>\n",
       "      <td>116.0</td>\n",
       "      <td>1490.00</td>\n",
       "      <td>1110.00</td>\n",
       "      <td>11.15</td>\n",
       "      <td>59.575001</td>\n",
       "      <td>0.788794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CO(GT)  PT08.S1(CO)  NMHC(GT)   C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  \\\n",
       "0     2.6      1360.00       150  11.881723        1045.50    166.0   \n",
       "1     2.0      1292.25       112   9.397165         954.75    103.0   \n",
       "2     2.2      1402.00        88   8.997817         939.25    131.0   \n",
       "3     2.2      1375.50        80   9.228796         948.25    172.0   \n",
       "4     1.6      1272.25        51   6.518224         835.50    131.0   \n",
       "\n",
       "   PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)      T         RH  \\\n",
       "0       1056.25    113.0       1692.00      1267.50  13.60  48.875001   \n",
       "1       1173.75     92.0       1558.75       972.25  13.30  47.700000   \n",
       "2       1140.00    114.0       1554.50      1074.00  11.90  53.975000   \n",
       "3       1092.00    122.0       1583.75      1203.25  11.00  60.000000   \n",
       "4       1205.00    116.0       1490.00      1110.00  11.15  59.575001   \n",
       "\n",
       "         AH  \n",
       "0  0.757754  \n",
       "1  0.725487  \n",
       "2  0.750239  \n",
       "3  0.786713  \n",
       "4  0.788794  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display a part of the data\n",
    "air_quality_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d400a420",
   "metadata": {},
   "source": [
    "#### Sanitizing the data and removing empty instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adda1ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first remove the rows for which there are missing values in the target feature\n",
    "air_quality = air_quality_full.loc[air_quality_full.iloc[:, 0]!=-200, :]\n",
    "# and the columns (features) for which there are more that 20% of missing values\n",
    "import numpy as np\n",
    "ndata, ncols = np.shape(air_quality) # number of data observations and number of columns in the dataframe\n",
    "pmissing = np.empty(ncols)         # An empty vector that will keep the percentage of missing values per feature\n",
    "for i in range(ncols):\n",
    "    pmissing[i] = (air_quality.iloc[:, i]==-200).sum()/ndata # Computes the percentage of missing values per column\n",
    "air_quality = air_quality.loc[:, pmissing < 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc013574",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)                 # Adding a seed to get consistent results\n",
    "index = np.random.permutation(ndata)  # We permute the indexes \n",
    "N = np.int64(np.round(0.70*ndata))    # We compute N, the number of training instances\n",
    "Nval = np.int64(np.round(0.15*ndata)) # We compute Nval, the number of validation instances   \n",
    "Ntest = ndata - N - Nval              # We compute Ntest, the number of test instances\n",
    "data_training_unproc = air_quality.iloc[index[0:N], :].copy() # Select the training data\n",
    "data_val_unproc = air_quality.iloc[index[N:N+Nval], :].copy() # Select the validation data\n",
    "data_test_unproc = air_quality.iloc[index[N+Nval:ndata], :].copy() # Select the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba81dda1",
   "metadata": {},
   "source": [
    "#### Preprocessing - Imputing missing values and standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be5eab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to impute missing values, standardise the dataset and return both transformed set as well as imputed values\n",
    "def preprocessing(dataset,values_in={}):\n",
    "    values = {}\n",
    "    dataset.replace(to_replace=-200, value=np.NaN, inplace=True)\n",
    "    for col in dataset.columns:\n",
    "        if col in values_in:\n",
    "            col_mean = values_in[col]['mean']\n",
    "            col_std_dev = values_in[col]['std_dev']\n",
    "        else:\n",
    "            col_mean = dataset[col].mean()\n",
    "            col_std_dev = dataset[col].std()\n",
    "            \n",
    "        dataset[col] = dataset[col].fillna(col_mean)\n",
    "        dataset[col]= (dataset[col]-col_mean)/(col_std_dev)\n",
    "        values[col] = {'mean':col_mean,'std_dev':col_std_dev}\n",
    "    return dataset, values\n",
    "\n",
    "#Preprocessing Training Data and saving imputed values\n",
    "data_training, values = preprocessing(data_training_unproc.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab41a40",
   "metadata": {},
   "source": [
    "#### Seperating the data into Design Matrix and Targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa95b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to seperate data into Targets y and Design Matrix X\n",
    "def seperate(dataset):\n",
    "    y = np.array(dataset.get('CO(GT)')).reshape(-1,1)\n",
    "    tmp_data = dataset.drop(columns='CO(GT)')\n",
    "    tmp_array = tmp_data.to_numpy()\n",
    "    X = np.hstack((np.ones_like(y), tmp_array))\n",
    "    return X,y\n",
    "\n",
    "#Seperating training data and targets\n",
    "XTrain, yTrain = seperate(data_training.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779bb535",
   "metadata": {},
   "source": [
    "#### Implementing Ridge Regression using Minibatch Gradient Descent and finding the optimal hyperparameters using Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93b95efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardise the validation data using the means and standard deviations computed from the training data.\n",
    "data_val, _ = preprocessing(data_val_unproc.copy(),values.copy())\n",
    "XVal, yVal = seperate(data_val.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2147619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a grid of values for the parameters ð›¾ and ðœ‚ using np.logspace and a grid of values for ð‘† using np.linspace.\n",
    "learn_rates = list(np.logspace(start =-2, stop = -1, num = 5)) \n",
    "regularization_params = list(np.logspace(start = -4, stop = -1, num = 5))\n",
    "datapoints = list(np.linspace(start = 16, stop = 80, num = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e72485c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learn_rate': 0.05623413251903491, 'regularization_param': 0.0001, 'batch_size': 48, 'rmse': 0.32331021437771945}\n"
     ]
    }
   ],
   "source": [
    "#Fucntion to Create batches from input data for Minibatch Gradient Descent\n",
    "def create_batch(XTrain,yTrain,data_size):\n",
    "    mini_batches = []\n",
    "    i = 0\n",
    "    data = np.hstack((XTrain,yTrain))\n",
    "    batches = data.shape[0] // data_size\n",
    "    for i in range(int(batches)):\n",
    "        mini_batch = data[i*data_size:(i+1)*data_size, :]\n",
    "        x = mini_batch[:, :-1]\n",
    "        y = mini_batch[:, -1].reshape((-1, 1))\n",
    "        mini_batches.append((x, y))\n",
    "    if data.shape[0] % data_size != 0:\n",
    "        mini_batch = data[-data_size:]\n",
    "        x = mini_batch[:, :-1]\n",
    "        y = mini_batch[:, -1].reshape((-1, 1))\n",
    "        mini_batches.append((x, y))\n",
    "    return mini_batches\n",
    "\n",
    "#fucntion to calculate RMSE\n",
    "def calc_rmse(XVal,w,yVal):\n",
    "    y_hat = np.dot(XVal,w)\n",
    "    rmse = np.sqrt(((yVal-y_hat)**2).mean())\n",
    "    return rmse\n",
    "\n",
    "#Function to Perform Minibatch Gradient Descent and calculate RMSE\n",
    "def mb_gradientdescent(XTrain,yTrain,learn_rate,regularization_param,data_size,XVal,yVal):\n",
    "    w = np.ones((XTrain.shape[1],1))\n",
    "    max_iterations = 200\n",
    "    for iteration in range(max_iterations):\n",
    "        mini_batches = create_batch(XTrain,yTrain,data_size)\n",
    "        for batch in mini_batches:\n",
    "            x, y = batch\n",
    "            w_old = w\n",
    "            f = np.dot(x,w_old)\n",
    "            loss = y - f\n",
    "            summ = np.dot(x.T,loss)\n",
    "            gradient = (2*learn_rate*summ)/data_size\n",
    "            temp = 1-(regularization_param*learn_rate)\n",
    "            w = (temp*w_old) + gradient\n",
    "    \n",
    "    rmse = calc_rmse(XVal,w,yVal)\n",
    "    return learn_rate,regularization_param,data_size,rmse\n",
    "\n",
    "lowest = {}  #Initialize dictionary Variable to store the lowest values.\n",
    "rmse_prev = 100 \n",
    "\n",
    "#Computing the Value of w for over each possible permutation of the three given hyperparameters \n",
    "for ds in datapoints:\n",
    "    for rp in regularization_params:\n",
    "        for lr in learn_rates:\n",
    "            #Perform GD and Calculate RMSE\n",
    "            learn_rate,regularization_param,data_size,rmse = mb_gradientdescent(\n",
    "                                                                    XTrain,\n",
    "                                                                    yTrain,\n",
    "                                                                    lr,\n",
    "                                                                    rp,\n",
    "                                                                    int(ds),\n",
    "                                                                    XVal,\n",
    "                                                                    yVal\n",
    "                                                                )\n",
    "            #Choose the values of ð›¾, ðœ‚ and ð‘† that lead to the lowest RMSE and save them.\n",
    "            if rmse < rmse_prev:\n",
    "                lowest = {\n",
    "                    'learn_rate':learn_rate,\n",
    "                    'regularization_param':regularization_param,\n",
    "                    'batch_size':data_size,\n",
    "                    'rmse':rmse}\n",
    "                \n",
    "                rmse_prev = rmse\n",
    "                \n",
    "#print the lowest parameters\n",
    "print(lowest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a20a3f0",
   "metadata": {},
   "source": [
    "#### Training the model using the best hyperparameters and predict over test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "755e3b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put together the original training and validation dataset\n",
    "new_data_train = pd.concat([data_training_unproc, data_val_unproc])\n",
    "\n",
    "#for each feature, impute the missing values with the mean values of the non-missing values \n",
    "#stardardise the new training set\n",
    "process_data_train, train_values = preprocessing(new_data_train.copy())\n",
    "XnTrain, ynTrain = seperate(process_data_train.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11d8f4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE over test data is : 0.34308048829970467\n"
     ]
    }
   ],
   "source": [
    "#Preprocess the test data by imputing the missing data and standardising it\n",
    "data_test, _ = preprocessing(data_test_unproc.copy(),train_values)\n",
    "XTest, yTest = seperate(data_test.copy())\n",
    "\n",
    "#Use the best values of ð›¾, ðœ‚ and ð‘† found in the validation set and train a new regularised linear model with stochastic gradient descent\n",
    "_,_,_,rmse = mb_gradientdescent(XnTrain,ynTrain,lowest['learn_rate'],lowest['regularization_param'],lowest['batch_size'],XTest,yTest)\n",
    "\n",
    "#Report the RMSE over the test data\n",
    "print(f'The RMSE over test data is : {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097c3fc5",
   "metadata": {},
   "source": [
    "### 2. Training a random forest for regression over the air quality dataset\n",
    "\n",
    "We now use random forests to predict air quality. The tree ensemble in random forests is built by training individual regression trees on different subsets of the training data and using a subset of the available features. For regression, the prediction is the average of the individual predictions of each tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9ec84e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c21ca2",
   "metadata": {},
   "source": [
    "#### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecbd5e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Employ the SimpleImputer method in Scikit-learn to impute the missing values in each column\n",
    "impute = SimpleImputer(missing_values=-200, strategy='mean')\n",
    "#Standardise the data by substracting the mean value for each feature and dividing the result by the standard deviation of each feature. Employ the StandardScaler method\n",
    "scale = StandardScaler()\n",
    "\n",
    "#Create a Pipeline that you can use to preprocess the data by filling missing values and then standardising the features\n",
    "estimators = [('impute', SimpleImputer(missing_values=-200, strategy='mean')), ('scale', StandardScaler())]\n",
    "pipe = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfeebdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Function to seperate dataset into data and targets\n",
    "def seperate2(dataset):\n",
    "    if type(dataset) is np.ndarray:\n",
    "        y = dataset[:,0].reshape(-1,1)\n",
    "        X = dataset[:,1:]\n",
    "    else:\n",
    "        y = dataset.get('CO(GT)')\n",
    "        tmp_data = dataset.drop(columns='CO(GT)')\n",
    "        X  = tmp_data\n",
    "    return X,y\n",
    "\n",
    "#Seperate Training and Validation Data into data and targets\n",
    "X_Train_rf, y_Train_rf = seperate2(data_training_unproc.copy().to_numpy())\n",
    "X_Val_rf, y_Val_rf = seperate2(data_val_unproc.copy().to_numpy())\n",
    "\n",
    "# fit the training data  \n",
    "pipe.fit(X_Train_rf.copy(),y_Train_rf.copy())\n",
    "\n",
    "#transform the validation data\n",
    "X_TTrain = pipe.transform(X_Train_rf.copy())\n",
    "X_TVal = pipe.transform(X_Val_rf.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b52299",
   "metadata": {},
   "source": [
    "#### Use cross-validation over the validation data to select the best set of paramaters for the random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbad9f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'max_features': 3, 'max_samples': 0.9772372209558107, 'n_estimators': 325}\n"
     ]
    }
   ],
   "source": [
    "#comine data and targets of training and validation dataset\n",
    "X = np.vstack((X_TTrain,X_TVal))\n",
    "y = np.hstack((y_Train_rf.ravel(),y_Val_rf.ravel()))\n",
    "\n",
    "#generate split index of train and validation data for combined data \n",
    "split_index = (np.append(np.full((X_TTrain.shape[0],), -1),np.full((X_TVal.shape[0],), 0))).reshape(-1,1)\n",
    "\n",
    "#Use PredefinedSplit to tell the cross validator which instances to use for training and which ones for validation\n",
    "split = PredefinedSplit(test_fold = split_index)\n",
    "\n",
    "#Create a grid of five values for each parameter                      \n",
    "n_estimators =  list(np.linspace(start = 100, stop = 1000,num = 5)) \n",
    "n_estimators =  [int(_) for _ in n_estimators]\n",
    "max_features = list(np.linspace(start = 3, stop = 6, num = 3))\n",
    "max_features =  [int(_) for _ in max_features]\n",
    "max_samples = list(np.logspace(start =-1, stop = -0.01, num = 5))\n",
    "\n",
    "#create the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators':n_estimators,\n",
    "    'max_features':max_features,\n",
    "    'max_samples':max_samples\n",
    "}\n",
    "\n",
    "#use GridSearchCV to perform Cross-Validation over the predefined split to find the best parameters\n",
    "gridsearch = GridSearchCV(\n",
    "    RandomForestRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    cv=split\n",
    ")\n",
    "\n",
    "gridsearch.fit(X,y)\n",
    "#print the best generated parameters\n",
    "print(f'The best parameters are {gridsearch.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d24527",
   "metadata": {},
   "source": [
    "#### Train a new model using the best parameters over the whole training data and report the prediction over the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5116fad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE over test data is : 0.4077750056194089\n"
     ]
    }
   ],
   "source": [
    "#combine training and validation data\n",
    "unify = pd.concat([data_training_unproc, data_val_unproc])\n",
    "\n",
    "#seperate the combined dataset into data targets \n",
    "X_final, y_final = seperate2(unify.copy())\n",
    "X_test_rf, y_test_rf = seperate2(data_test_unproc.copy())\n",
    "\n",
    "#Create and apply a new preprocessing pipeline for imputing and standardising the data\n",
    "estimators = [('impute', SimpleImputer(missing_values=-200, strategy='mean')), ('scale', StandardScaler())]\n",
    "pipe2 = Pipeline(estimators)\n",
    "\n",
    "#fit the pipeline on whole training datasete\n",
    "pipe2.fit(X_final.copy(), y_final.copy()) \n",
    "\n",
    "#use the pipeline to transform the whole trianing and testing dataset\n",
    "X_Tfinal = pipe.transform(X_final.copy())\n",
    "X_Ttest = pipe.transform(X_test_rf.copy())\n",
    "\n",
    "#Fit a random forest regression model to the training data using the best parameters found in Question 2.c\n",
    "rf = RandomForestRegressor(max_features=gridsearch.best_params_['max_features'], max_samples=gridsearch.best_params_['max_samples'], n_estimators=gridsearch.best_params_['n_estimators'])\n",
    "rf.fit(X_Tfinal,y_final)\n",
    "\n",
    "#Compute the RMSE over the test data and report the result\n",
    "predict = rf.predict(X_Ttest)\n",
    "rmse = mean_squared_error(y_test_rf,predict, squared=False)\n",
    "print(f'The RMSE over test data is : {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee8ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
